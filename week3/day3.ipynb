{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayiYEglO9T8Z"
      },
      "source": [
        "# Tokenizers\n",
        "\n",
        "For this Colab session, we explore the world of Tokenizers\n",
        "\n",
        "You can run this notebook on a free CPU, or locally on your box if you prefer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK1rDJXe2KjS"
      },
      "source": [
        "## Reminder: 2 important pro-tips for using Colab:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "The top of every colab has some pip installs. You may receive errors from pip when you run this, such as:\n",
        "\n",
        "> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
        "\n",
        "These pip compatibility errors can be safely ignored; and while it's tempting to try to fix them by changing version numbers, that will actually introduce real problems!\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "In the middle of running a Colab, you might get an error like this:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "This is a super-misleading error message! Please don't try changing versions of packages...\n",
        "\n",
        "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
        "\n",
        "1. Kernel menu >> Disconnect and delete runtime\n",
        "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
        "3. Connect to a new T4 using the button at the top right\n",
        "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
        "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
        "\n",
        "And all should work great - otherwise, ask me!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w8vZjnoO2a-5"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C9zvDGWD5pKp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyKWKWSw7Iqp"
      },
      "source": [
        "# Sign in to Hugging Face\n",
        "\n",
        "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
        "\n",
        "**IMPORTANT** when you create your HuggingFace API key, please be sure to select read/write permissions for your key by clicking on the WRITE tab, otherwise you may get problems later.\n",
        "\n",
        "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
        "`HF_TOKEN = your_token`\n",
        "\n",
        "3. Execute the cell below to log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xd7cEDUC6Lkq"
      },
      "outputs": [],
      "source": [
        "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSiYqPn87msu"
      },
      "source": [
        "# Accessing Llama 3.1 from Meta\n",
        "\n",
        "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
        "\n",
        "Visit their model instructions page in Hugging Face:\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
        "\n",
        "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
        "\n",
        "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole 3.1 family of models. For whatever reason, occasionally Meta doesn't approve access. If that happens to you, please follow [this](https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8?usp=sharing) troubleshooting.\n",
        "\n",
        "If the next cell gives you an error, then please check:  \n",
        "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
        "2. Did you set up your API key with full read and write permissions?\n",
        "3. If you visit the Llama3.1 page with the link above, does it show that you have access to the model near the top?\n",
        "\n",
        "I've also set up this troubleshooting colab to try to diagnose any HuggingFace connectivity issues:  \n",
        "https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "swoiXwUb7RoA"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2883b90c0cd8421da2f5b4d5cc791316",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73c848d960f7475987bbac0f4b2657a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb0327b62b674136922130fd48ef8692",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kgJTmHNm8Ui4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[128000,\n",
              " 40,\n",
              " 1097,\n",
              " 12304,\n",
              " 311,\n",
              " 1501,\n",
              " 9857,\n",
              " 12509,\n",
              " 304,\n",
              " 1957,\n",
              " 311,\n",
              " 856,\n",
              " 445,\n",
              " 11237,\n",
              " 25175]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QHnp79Ig8vPT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rtptNsDf83RC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|>I am excited to show Tokenizers in action to my LLM engineers'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZlQT65oz8-aF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>',\n",
              " 'I',\n",
              " ' am',\n",
              " ' excited',\n",
              " ' to',\n",
              " ' show',\n",
              " ' Token',\n",
              " 'izers',\n",
              " ' in',\n",
              " ' action',\n",
              " ' to',\n",
              " ' my',\n",
              " ' L',\n",
              " 'LM',\n",
              " ' engineers']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.batch_decode(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y7LTUIlD9Gdm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<|begin_of_text|>': 128000,\n",
              " '<|end_of_text|>': 128001,\n",
              " '<|reserved_special_token_0|>': 128002,\n",
              " '<|reserved_special_token_1|>': 128003,\n",
              " '<|finetune_right_pad_id|>': 128004,\n",
              " '<|reserved_special_token_2|>': 128005,\n",
              " '<|start_header_id|>': 128006,\n",
              " '<|end_header_id|>': 128007,\n",
              " '<|eom_id|>': 128008,\n",
              " '<|eot_id|>': 128009,\n",
              " '<|python_tag|>': 128010,\n",
              " '<|reserved_special_token_3|>': 128011,\n",
              " '<|reserved_special_token_4|>': 128012,\n",
              " '<|reserved_special_token_5|>': 128013,\n",
              " '<|reserved_special_token_6|>': 128014,\n",
              " '<|reserved_special_token_7|>': 128015,\n",
              " '<|reserved_special_token_8|>': 128016,\n",
              " '<|reserved_special_token_9|>': 128017,\n",
              " '<|reserved_special_token_10|>': 128018,\n",
              " '<|reserved_special_token_11|>': 128019,\n",
              " '<|reserved_special_token_12|>': 128020,\n",
              " '<|reserved_special_token_13|>': 128021,\n",
              " '<|reserved_special_token_14|>': 128022,\n",
              " '<|reserved_special_token_15|>': 128023,\n",
              " '<|reserved_special_token_16|>': 128024,\n",
              " '<|reserved_special_token_17|>': 128025,\n",
              " '<|reserved_special_token_18|>': 128026,\n",
              " '<|reserved_special_token_19|>': 128027,\n",
              " '<|reserved_special_token_20|>': 128028,\n",
              " '<|reserved_special_token_21|>': 128029,\n",
              " '<|reserved_special_token_22|>': 128030,\n",
              " '<|reserved_special_token_23|>': 128031,\n",
              " '<|reserved_special_token_24|>': 128032,\n",
              " '<|reserved_special_token_25|>': 128033,\n",
              " '<|reserved_special_token_26|>': 128034,\n",
              " '<|reserved_special_token_27|>': 128035,\n",
              " '<|reserved_special_token_28|>': 128036,\n",
              " '<|reserved_special_token_29|>': 128037,\n",
              " '<|reserved_special_token_30|>': 128038,\n",
              " '<|reserved_special_token_31|>': 128039,\n",
              " '<|reserved_special_token_32|>': 128040,\n",
              " '<|reserved_special_token_33|>': 128041,\n",
              " '<|reserved_special_token_34|>': 128042,\n",
              " '<|reserved_special_token_35|>': 128043,\n",
              " '<|reserved_special_token_36|>': 128044,\n",
              " '<|reserved_special_token_37|>': 128045,\n",
              " '<|reserved_special_token_38|>': 128046,\n",
              " '<|reserved_special_token_39|>': 128047,\n",
              " '<|reserved_special_token_40|>': 128048,\n",
              " '<|reserved_special_token_41|>': 128049,\n",
              " '<|reserved_special_token_42|>': 128050,\n",
              " '<|reserved_special_token_43|>': 128051,\n",
              " '<|reserved_special_token_44|>': 128052,\n",
              " '<|reserved_special_token_45|>': 128053,\n",
              " '<|reserved_special_token_46|>': 128054,\n",
              " '<|reserved_special_token_47|>': 128055,\n",
              " '<|reserved_special_token_48|>': 128056,\n",
              " '<|reserved_special_token_49|>': 128057,\n",
              " '<|reserved_special_token_50|>': 128058,\n",
              " '<|reserved_special_token_51|>': 128059,\n",
              " '<|reserved_special_token_52|>': 128060,\n",
              " '<|reserved_special_token_53|>': 128061,\n",
              " '<|reserved_special_token_54|>': 128062,\n",
              " '<|reserved_special_token_55|>': 128063,\n",
              " '<|reserved_special_token_56|>': 128064,\n",
              " '<|reserved_special_token_57|>': 128065,\n",
              " '<|reserved_special_token_58|>': 128066,\n",
              " '<|reserved_special_token_59|>': 128067,\n",
              " '<|reserved_special_token_60|>': 128068,\n",
              " '<|reserved_special_token_61|>': 128069,\n",
              " '<|reserved_special_token_62|>': 128070,\n",
              " '<|reserved_special_token_63|>': 128071,\n",
              " '<|reserved_special_token_64|>': 128072,\n",
              " '<|reserved_special_token_65|>': 128073,\n",
              " '<|reserved_special_token_66|>': 128074,\n",
              " '<|reserved_special_token_67|>': 128075,\n",
              " '<|reserved_special_token_68|>': 128076,\n",
              " '<|reserved_special_token_69|>': 128077,\n",
              " '<|reserved_special_token_70|>': 128078,\n",
              " '<|reserved_special_token_71|>': 128079,\n",
              " '<|reserved_special_token_72|>': 128080,\n",
              " '<|reserved_special_token_73|>': 128081,\n",
              " '<|reserved_special_token_74|>': 128082,\n",
              " '<|reserved_special_token_75|>': 128083,\n",
              " '<|reserved_special_token_76|>': 128084,\n",
              " '<|reserved_special_token_77|>': 128085,\n",
              " '<|reserved_special_token_78|>': 128086,\n",
              " '<|reserved_special_token_79|>': 128087,\n",
              " '<|reserved_special_token_80|>': 128088,\n",
              " '<|reserved_special_token_81|>': 128089,\n",
              " '<|reserved_special_token_82|>': 128090,\n",
              " '<|reserved_special_token_83|>': 128091,\n",
              " '<|reserved_special_token_84|>': 128092,\n",
              " '<|reserved_special_token_85|>': 128093,\n",
              " '<|reserved_special_token_86|>': 128094,\n",
              " '<|reserved_special_token_87|>': 128095,\n",
              " '<|reserved_special_token_88|>': 128096,\n",
              " '<|reserved_special_token_89|>': 128097,\n",
              " '<|reserved_special_token_90|>': 128098,\n",
              " '<|reserved_special_token_91|>': 128099,\n",
              " '<|reserved_special_token_92|>': 128100,\n",
              " '<|reserved_special_token_93|>': 128101,\n",
              " '<|reserved_special_token_94|>': 128102,\n",
              " '<|reserved_special_token_95|>': 128103,\n",
              " '<|reserved_special_token_96|>': 128104,\n",
              " '<|reserved_special_token_97|>': 128105,\n",
              " '<|reserved_special_token_98|>': 128106,\n",
              " '<|reserved_special_token_99|>': 128107,\n",
              " '<|reserved_special_token_100|>': 128108,\n",
              " '<|reserved_special_token_101|>': 128109,\n",
              " '<|reserved_special_token_102|>': 128110,\n",
              " '<|reserved_special_token_103|>': 128111,\n",
              " '<|reserved_special_token_104|>': 128112,\n",
              " '<|reserved_special_token_105|>': 128113,\n",
              " '<|reserved_special_token_106|>': 128114,\n",
              " '<|reserved_special_token_107|>': 128115,\n",
              " '<|reserved_special_token_108|>': 128116,\n",
              " '<|reserved_special_token_109|>': 128117,\n",
              " '<|reserved_special_token_110|>': 128118,\n",
              " '<|reserved_special_token_111|>': 128119,\n",
              " '<|reserved_special_token_112|>': 128120,\n",
              " '<|reserved_special_token_113|>': 128121,\n",
              " '<|reserved_special_token_114|>': 128122,\n",
              " '<|reserved_special_token_115|>': 128123,\n",
              " '<|reserved_special_token_116|>': 128124,\n",
              " '<|reserved_special_token_117|>': 128125,\n",
              " '<|reserved_special_token_118|>': 128126,\n",
              " '<|reserved_special_token_119|>': 128127,\n",
              " '<|reserved_special_token_120|>': 128128,\n",
              " '<|reserved_special_token_121|>': 128129,\n",
              " '<|reserved_special_token_122|>': 128130,\n",
              " '<|reserved_special_token_123|>': 128131,\n",
              " '<|reserved_special_token_124|>': 128132,\n",
              " '<|reserved_special_token_125|>': 128133,\n",
              " '<|reserved_special_token_126|>': 128134,\n",
              " '<|reserved_special_token_127|>': 128135,\n",
              " '<|reserved_special_token_128|>': 128136,\n",
              " '<|reserved_special_token_129|>': 128137,\n",
              " '<|reserved_special_token_130|>': 128138,\n",
              " '<|reserved_special_token_131|>': 128139,\n",
              " '<|reserved_special_token_132|>': 128140,\n",
              " '<|reserved_special_token_133|>': 128141,\n",
              " '<|reserved_special_token_134|>': 128142,\n",
              " '<|reserved_special_token_135|>': 128143,\n",
              " '<|reserved_special_token_136|>': 128144,\n",
              " '<|reserved_special_token_137|>': 128145,\n",
              " '<|reserved_special_token_138|>': 128146,\n",
              " '<|reserved_special_token_139|>': 128147,\n",
              " '<|reserved_special_token_140|>': 128148,\n",
              " '<|reserved_special_token_141|>': 128149,\n",
              " '<|reserved_special_token_142|>': 128150,\n",
              " '<|reserved_special_token_143|>': 128151,\n",
              " '<|reserved_special_token_144|>': 128152,\n",
              " '<|reserved_special_token_145|>': 128153,\n",
              " '<|reserved_special_token_146|>': 128154,\n",
              " '<|reserved_special_token_147|>': 128155,\n",
              " '<|reserved_special_token_148|>': 128156,\n",
              " '<|reserved_special_token_149|>': 128157,\n",
              " '<|reserved_special_token_150|>': 128158,\n",
              " '<|reserved_special_token_151|>': 128159,\n",
              " '<|reserved_special_token_152|>': 128160,\n",
              " '<|reserved_special_token_153|>': 128161,\n",
              " '<|reserved_special_token_154|>': 128162,\n",
              " '<|reserved_special_token_155|>': 128163,\n",
              " '<|reserved_special_token_156|>': 128164,\n",
              " '<|reserved_special_token_157|>': 128165,\n",
              " '<|reserved_special_token_158|>': 128166,\n",
              " '<|reserved_special_token_159|>': 128167,\n",
              " '<|reserved_special_token_160|>': 128168,\n",
              " '<|reserved_special_token_161|>': 128169,\n",
              " '<|reserved_special_token_162|>': 128170,\n",
              " '<|reserved_special_token_163|>': 128171,\n",
              " '<|reserved_special_token_164|>': 128172,\n",
              " '<|reserved_special_token_165|>': 128173,\n",
              " '<|reserved_special_token_166|>': 128174,\n",
              " '<|reserved_special_token_167|>': 128175,\n",
              " '<|reserved_special_token_168|>': 128176,\n",
              " '<|reserved_special_token_169|>': 128177,\n",
              " '<|reserved_special_token_170|>': 128178,\n",
              " '<|reserved_special_token_171|>': 128179,\n",
              " '<|reserved_special_token_172|>': 128180,\n",
              " '<|reserved_special_token_173|>': 128181,\n",
              " '<|reserved_special_token_174|>': 128182,\n",
              " '<|reserved_special_token_175|>': 128183,\n",
              " '<|reserved_special_token_176|>': 128184,\n",
              " '<|reserved_special_token_177|>': 128185,\n",
              " '<|reserved_special_token_178|>': 128186,\n",
              " '<|reserved_special_token_179|>': 128187,\n",
              " '<|reserved_special_token_180|>': 128188,\n",
              " '<|reserved_special_token_181|>': 128189,\n",
              " '<|reserved_special_token_182|>': 128190,\n",
              " '<|reserved_special_token_183|>': 128191,\n",
              " '<|reserved_special_token_184|>': 128192,\n",
              " '<|reserved_special_token_185|>': 128193,\n",
              " '<|reserved_special_token_186|>': 128194,\n",
              " '<|reserved_special_token_187|>': 128195,\n",
              " '<|reserved_special_token_188|>': 128196,\n",
              " '<|reserved_special_token_189|>': 128197,\n",
              " '<|reserved_special_token_190|>': 128198,\n",
              " '<|reserved_special_token_191|>': 128199,\n",
              " '<|reserved_special_token_192|>': 128200,\n",
              " '<|reserved_special_token_193|>': 128201,\n",
              " '<|reserved_special_token_194|>': 128202,\n",
              " '<|reserved_special_token_195|>': 128203,\n",
              " '<|reserved_special_token_196|>': 128204,\n",
              " '<|reserved_special_token_197|>': 128205,\n",
              " '<|reserved_special_token_198|>': 128206,\n",
              " '<|reserved_special_token_199|>': 128207,\n",
              " '<|reserved_special_token_200|>': 128208,\n",
              " '<|reserved_special_token_201|>': 128209,\n",
              " '<|reserved_special_token_202|>': 128210,\n",
              " '<|reserved_special_token_203|>': 128211,\n",
              " '<|reserved_special_token_204|>': 128212,\n",
              " '<|reserved_special_token_205|>': 128213,\n",
              " '<|reserved_special_token_206|>': 128214,\n",
              " '<|reserved_special_token_207|>': 128215,\n",
              " '<|reserved_special_token_208|>': 128216,\n",
              " '<|reserved_special_token_209|>': 128217,\n",
              " '<|reserved_special_token_210|>': 128218,\n",
              " '<|reserved_special_token_211|>': 128219,\n",
              " '<|reserved_special_token_212|>': 128220,\n",
              " '<|reserved_special_token_213|>': 128221,\n",
              " '<|reserved_special_token_214|>': 128222,\n",
              " '<|reserved_special_token_215|>': 128223,\n",
              " '<|reserved_special_token_216|>': 128224,\n",
              " '<|reserved_special_token_217|>': 128225,\n",
              " '<|reserved_special_token_218|>': 128226,\n",
              " '<|reserved_special_token_219|>': 128227,\n",
              " '<|reserved_special_token_220|>': 128228,\n",
              " '<|reserved_special_token_221|>': 128229,\n",
              " '<|reserved_special_token_222|>': 128230,\n",
              " '<|reserved_special_token_223|>': 128231,\n",
              " '<|reserved_special_token_224|>': 128232,\n",
              " '<|reserved_special_token_225|>': 128233,\n",
              " '<|reserved_special_token_226|>': 128234,\n",
              " '<|reserved_special_token_227|>': 128235,\n",
              " '<|reserved_special_token_228|>': 128236,\n",
              " '<|reserved_special_token_229|>': 128237,\n",
              " '<|reserved_special_token_230|>': 128238,\n",
              " '<|reserved_special_token_231|>': 128239,\n",
              " '<|reserved_special_token_232|>': 128240,\n",
              " '<|reserved_special_token_233|>': 128241,\n",
              " '<|reserved_special_token_234|>': 128242,\n",
              " '<|reserved_special_token_235|>': 128243,\n",
              " '<|reserved_special_token_236|>': 128244,\n",
              " '<|reserved_special_token_237|>': 128245,\n",
              " '<|reserved_special_token_238|>': 128246,\n",
              " '<|reserved_special_token_239|>': 128247,\n",
              " '<|reserved_special_token_240|>': 128248,\n",
              " '<|reserved_special_token_241|>': 128249,\n",
              " '<|reserved_special_token_242|>': 128250,\n",
              " '<|reserved_special_token_243|>': 128251,\n",
              " '<|reserved_special_token_244|>': 128252,\n",
              " '<|reserved_special_token_245|>': 128253,\n",
              " '<|reserved_special_token_246|>': 128254,\n",
              " '<|reserved_special_token_247|>': 128255}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenizer.vocab\n",
        "tokenizer.get_added_vocab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kke10gYj_U87"
      },
      "source": [
        "# Instruct variants of models\n",
        "\n",
        "Many models have a variant that has been trained for use in Chats.  \n",
        "These are typically labelled with the word \"Instruct\" at the end.  \n",
        "They have been trained to expect prompts with a particular format that includes system, user and assistant prompts.  \n",
        "\n",
        "There is a utility method `apply_chat_template` that will convert from the messages list format we are familiar with, into the right input prompt for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4DJs4UPx9XfE"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "268d1c38b8ac4d6eaa085a71e4a98537",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb418f8899d44e98836f450c657b8e8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "988952603a134ecb884440905ffe96d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uKJjVJ9T_GFn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlmNvvb-AIKt"
      },
      "source": [
        "# Trying new models\n",
        "\n",
        "We will now work with 3 models:\n",
        "\n",
        "Phi3 from Microsoft\n",
        "Qwen2 from Alibaba Cloud\n",
        "Starcoder2 from BigCode (ServiceNow + HuggingFace + NVidia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FWUMdt_iAZQZ"
      },
      "outputs": [],
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WM9tU_ZnAbkR"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4c89484f71b4245aa365ff8fc7badbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d77e3eff1fd43b297adb5f489a409a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc3dba0c04af497c9596ecd4726eb399",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b07a9eda5214ea695739627b042eae8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27374544be8b4aa587eb509eb9042d29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[128000, 40, 1097, 12304, 311, 1501, 9857, 12509, 304, 1957, 311, 856, 445, 11237, 25175]\n",
            "\n",
            "['I', 'am', 'excited', 'to', 'show', 'Token', 'izers', 'in', 'action', 'to', 'my', 'L', 'LM', 'engine', 'ers']\n"
          ]
        }
      ],
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4CrdGSBZAxx9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "\n",
            "<|system|>\n",
            "You are a helpful assistant<|end|>\n",
            "<|user|>\n",
            "Tell a light-hearted joke for a room of Data Scientists<|end|>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yr16p4HSA2A4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7db7565322e49f99ba7ed8c2eeff4ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebda26e302764f8eadfdff7be7cca09d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "423fbdb6fa0c4077830047c01d8b7f95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "495b6e40a488401db8c03cfbaa5a2563",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[128000, 40, 1097, 12304, 311, 1501, 9857, 12509, 304, 1957, 311, 856, 445, 11237, 25175]\n",
            "\n",
            "[306, 626, 24173, 304, 1510, 25159, 19427, 297, 3158, 304, 590, 365, 26369, 6012, 414]\n",
            "\n",
            "[40, 1079, 12035, 311, 1473, 9660, 12230, 304, 1917, 311, 847, 444, 10994, 24198]\n"
          ]
        }
      ],
      "source": [
        "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "print(phi3_tokenizer.encode(text))\n",
        "print()\n",
        "print(qwen2_tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sQ5wFS1oBdEN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "\n",
            "<|system|>\n",
            "You are a helpful assistant<|end|>\n",
            "<|user|>\n",
            "Tell a light-hearted joke for a room of Data Scientists<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "\n",
            "<|im_start|>system\n",
            "You are a helpful assistant<|im_end|>\n",
            "<|im_start|>user\n",
            "Tell a light-hearted joke for a room of Data Scientists<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_GGe6hzSBkBg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de31f6b3222a44dd9bdbc16f447f4d7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.88k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9c8c9530d7437295fb8a83d27a9c65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92afef3d6a5c4406beebafb054fe8cd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c9c908496964094b775d8f8e48a03c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bdc0672d50044e48d71214910195409",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "222=\n",
            "\n",
            "610=def\n",
            "17966= hello\n",
            "100=_\n",
            "5879=world\n",
            "45=(\n",
            "6427=person\n",
            "731=):\n",
            "353=\n",
            " \n",
            "1489= print\n",
            "459=(\"\n",
            "8302=Hello\n",
            "411=\",\n",
            "4944= person\n",
            "46=)\n",
            "222=\n",
            "\n"
          ]
        }
      ],
      "source": [
        "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
        "code = \"\"\"\n",
        "def hello_world(person):\n",
        "  print(\"Hello\", person)\n",
        "\"\"\"\n",
        "tokens = starcoder2_tokenizer.encode(code)\n",
        "for token in tokens:\n",
        "  print(f\"{token}={starcoder2_tokenizer.decode(token)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
